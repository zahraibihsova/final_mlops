ðŸ“¦ Getting Started
Prerequisites

Docker & Docker Compose installed

Python 3.13+ (for local testing without containers)

-How to run
1. Clone the repo:
git clone <your-repo-url>
cd <repo-root>
2. Start containers:
docker-compose up -d --build
Access services:

Frontend â†’ http://localhost:8501 

Backend Swagger â†’ http://localhost:8000/docs (http://127.0.0.1:8000/docs#/)


Usage

Upload CSV or Excel files in the frontend

Frontend calls backend at http://backend:8000/predict

Receive prediction results in real-time


CI/CD Deployment

GitHub Actions self-hosted runner on EC2

Docker Compose for automated container orchestration

Jobs include:

Code linting with Ruff

Docker image build

Deployment to EC2 containers

# FINAL_MLOPS

![Build Status](https://github.com/zahraibihsova/final_mlops/actions/workflows/ci-build.yaml/badge.svg)

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![3.12](https://img.shields.io/badge/Python-3.12-green.svg)](https://shields.io/)

---

Preproducible ML project scaffold powered by uv

## Structure
------------

    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”œâ”€â”€ data
    â”‚   â”œâ”€â”€ external       <- Data from third party sources.
    â”‚   â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚   â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚
    â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    â”‚                         the creator's initials, and a short `-` delimited description, e.g.
    â”‚                         `1.0-jqp-initial-data-exploration`.
    â”‚
    â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    â”‚   â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ uv.lock   <- The requirements file for reproducing the analysis environment, e.g.
    â”‚                         generated with `uv lock > uv.lock`
    â”‚
    â”œâ”€â”€ pyptoject.toml    <- makes project uv installable (uv installs) so src can be imported
    â”œâ”€â”€ src                <- Source code for use in this project.
    â”‚   â”œâ”€â”€ __init__.py    <- Makes src a Python module
    â”‚   â”‚
    â”‚   â”œâ”€â”€ data           <- Scripts to download or generate data
    â”‚   â”‚   â””â”€â”€ make_dataset.py
    â”‚   â”‚
    â”‚   â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling
    â”‚   â”‚   â””â”€â”€ build_features.py
    â”‚   â”‚
    â”‚   â”œâ”€â”€ models         <- Scripts to train models and then use trained models to make
    â”‚   â”‚   â”‚                 predictions
    â”‚   â”‚   â”œâ”€â”€ predict_model.py
    â”‚   â”‚   â””â”€â”€ train_model.py
    â”‚   â”‚
    â”‚   â””â”€â”€ visualization  <- Scripts to create exploratory and results oriented visualizations
    â”‚       â””â”€â”€ visualize.py


--------


## Getting started (uv)
```bash
# create venv and sync (will create uv.lock)
uv sync

# add a runtime dependency
uv add numpy

# run code
uv run python -m src.models.train_model
```

## Code quality (ruff, isort, black via uvx)
### Run tools in ephemeral envs â€” no dev dependencies added to your project.

#### Lint (no changes)
```bash
# Lint entire repo
uvx ruff check .
```

#### Auto-fix
```bash
# 1) Sort imports
uvx isort .

# 2) Format code
uvx black .

# 3) Apply Ruffâ€™s safe fixes (entire repo)
uvx ruff check --fix .
```
> Also remove unused imports/variables:
> ```bash
> uvx ruff check --fix --unsafe-fixes .
> ```
